{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n\n!pip install unsloth # install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T18:05:49.620067Z","iopub.execute_input":"2025-05-01T18:05:49.620425Z","iopub.status.idle":"2025-05-01T18:08:31.055268Z","shell.execute_reply.started":"2025-05-01T18:05:49.620397Z","shell.execute_reply":"2025-05-01T18:08:31.054302Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n# Hugging Face modules\nfrom huggingface_hub import login # Lets you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T18:08:43.741116Z","iopub.execute_input":"2025-05-01T18:08:43.741484Z","iopub.status.idle":"2025-05-01T18:09:13.564509Z","shell.execute_reply.started":"2025-05-01T18:08:43.741456Z","shell.execute_reply":"2025-05-01T18:09:13.563412Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Initialize Hugging Face & WnB tokens\nuser_secrets = UserSecretsClient() # from kaggle_secrets import UserSecretsClient\nhugging_face_token = user_secrets.get_secret(\"HuggingfaceAPI\")\nwnb_token = user_secrets.get_secret(\"w&b\")\n\n# Login to Hugging Face\nlogin(hugging_face_token) # from huggingface_hub import login\n\n# Login to WnB\nwandb.login(key=wnb_token) # import wandb\nrun = wandb.init(\n    project='meta-llama-Llama-3.1-8B', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T18:09:48.164364Z","iopub.execute_input":"2025-05-01T18:09:48.164674Z","iopub.status.idle":"2025-05-01T18:09:48.546437Z","shell.execute_reply.started":"2025-05-01T18:09:48.164654Z","shell.execute_reply":"2025-05-01T18:09:48.545581Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Set parameters\nmax_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # Set to default \nload_in_4bit = True # Enables 4 bit quantization — a memory saving optimization \n\n# Load the DeepSeek R1 model and tokenizer using unsloth — imported using: from unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"meta-llama/Llama-3.1-8B\",  # Load the pre-trained DeepSeek R1 model (8B parameter version)\n    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n    token=hugging_face_token, # Use hugging face token\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T18:10:28.709025Z","iopub.execute_input":"2025-05-01T18:10:28.709348Z","iopub.status.idle":"2025-05-01T18:11:35.700535Z","shell.execute_reply.started":"2025-05-01T18:10:28.709325Z","shell.execute_reply":"2025-05-01T18:11:35.699657Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.4.4: Fast Llama patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"759b181b5ea9441fa82e8fa1261feb39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9553912ab9146b0863aa0c4338a93f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58a8c61a6f5941c799ce9c27df0f2a9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0383c1c5f2474a5e894c8c1e8ec5bec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a39980a81132470d940b39ed2472758a"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Define a system prompt under prompt_style \nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T18:12:00.389152Z","iopub.execute_input":"2025-05-01T18:12:00.389508Z","iopub.status.idle":"2025-05-01T18:12:00.394527Z","shell.execute_reply.started":"2025-05-01T18:12:00.389483Z","shell.execute_reply":"2025-05-01T18:12:00.393648Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Creating a test medical question for inference\nquestion = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or \n              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, \n              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n\n# Enable optimized inference mode for Unsloth models (improves speed and efficiency)\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n\n# Format the question using the structured prompt (`prompt_style`) and tokenize it\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  # Convert input to PyTorch tensor & move to GPU\n\n# Generate a response using the model\noutputs = model.generate(\n    input_ids=inputs.input_ids, # Tokenized input question\n    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n    max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n    use_cache=True, # Enable caching for faster inference\n)\n\n# Decode the generated output tokens into human-readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the relevant response part (after \"### Response:\")\nprint(response[0].split(\"### Response:\")[1])  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T18:12:07.986488Z","iopub.execute_input":"2025-05-01T18:12:07.986894Z","iopub.status.idle":"2025-05-01T18:12:24.141281Z","shell.execute_reply.started":"2025-05-01T18:12:07.986855Z","shell.execute_reply":"2025-05-01T18:12:24.140266Z"}},"outputs":[{"name":"stdout","text":"\n<think>1. First, the patient has a history of involuntary urine loss during activities like coughing or sneezing but no leakage at night. This suggests that the patient may have an overactive bladder. Overactive bladder is characterized by symptoms such as urinary urgency, frequency, and nocturia. These symptoms can be caused by a variety of factors, including infection, neurogenic bladder, or bladder outlet obstruction. 2. The gynecological exam and Q-tip test suggest that the patient has a prolapsed bladder. Prolapsed bladder is a condition where the bladder protrudes through the opening of the vagina. This can be caused by weakened pelvic floor muscles, obesity, or chronic constipation. 3. Cystometry is a test that measures the volume of urine in the bladder and the pressure generated by the detrusor muscle during bladder filling. This test can be used to diagnose overactive bladder and prolapsed bladder. 4. Based on the findings of the gynecological exam and Q-tip test, the patient may have a combination of overactive bladder and prolapsed bladder. The cystometry test will most likely reveal a residual volume of urine in the bladder and detrusor contractions.</think><|end_of_text|>\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Creating a test medical question for inference\nquestion = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue, \n              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative, \n              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium. \n              What is the most likely predisposing factor for this patient's condition?\"\"\"\n\n# Enable optimized inference mode for Unsloth models (improves speed and efficiency)\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n\n# Format the question using the structured prompt (`prompt_style`) and tokenize it\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  # Convert input to PyTorch tensor & move to GPU\n\n# Generate a response using the model\noutputs = model.generate(\n    input_ids=inputs.input_ids, # Tokenized input question\n    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n    max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n    use_cache=True, # Enable caching for faster inference\n)\n\n# Decode the generated output tokens into human-readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the relevant response part (after \"### Response:\")\nprint(response[0].split(\"### Response:\")[1])  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T18:12:44.716125Z","iopub.execute_input":"2025-05-01T18:12:44.716504Z","iopub.status.idle":"2025-05-01T18:12:54.728506Z","shell.execute_reply.started":"2025-05-01T18:12:44.716474Z","shell.execute_reply":"2025-05-01T18:12:54.727622Z"}},"outputs":[{"name":"stdout","text":"\n<think> \n\n### Answer:\nThe most likely predisposing factor for this patient's condition is endocarditis. \n              Endocarditis is an infection of the inner lining of the heart chambers and valves. \n              The most common cause of endocarditis is a bacterial infection, which can be caused by a variety of different bacteria. \n              The most common type of bacteria that causes endocarditis is Streptococcus viridans, which is a type of gram-positive, catalase-negative, gamma-hemolytic cocci in chains. \n              Streptococcus viridans is also known to grow in a 6.5% NaCl medium, which is why the patient's blood cultures indicated the presence of this bacteria. \n              The presence of a vegetation on the aortic valve is also a common symptom of endocarditis. \n              The vegetation is a mass of bacteria and fibrin that forms on the surface of the valve, and can cause damage to the valve and lead to complications such as heart failure. \n              Therefore, the most likely predisposing factor for this patient's condition is endocarditis.\n\n</think>\n\n<|end_of_text|>\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Updated training prompt style to add </think> tag \ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:21:36.988470Z","iopub.execute_input":"2025-05-01T16:21:36.988809Z","iopub.status.idle":"2025-05-01T16:21:36.993233Z","shell.execute_reply.started":"2025-05-01T16:21:36.988779Z","shell.execute_reply":"2025-05-01T16:21:36.992449Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Download the dataset using Hugging Face — function imported using from datasets import load_dataset\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True) # Keep only first 500 rows\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:21:39.667294Z","iopub.execute_input":"2025-05-01T16:21:39.667578Z","iopub.status.idle":"2025-05-01T16:21:42.852975Z","shell.execute_reply.started":"2025-05-01T16:21:39.667557Z","shell.execute_reply":"2025-05-01T16:21:42.852261Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da10c7cc43e14e948e53848d12859be4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"medical_o1_sft.json:   0%|          | 0.00/58.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bebc8985e15478bb757a0c99f4d970c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/19704 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"974297249a1845d2a259315358d4ceea"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Question', 'Complex_CoT', 'Response'],\n    num_rows: 500\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Show an entry from the dataset\ndataset[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:21:46.456106Z","iopub.execute_input":"2025-05-01T16:21:46.456397Z","iopub.status.idle":"2025-05-01T16:21:46.463949Z","shell.execute_reply.started":"2025-05-01T16:21:46.456374Z","shell.execute_reply":"2025-05-01T16:21:46.463260Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'Question': 'A 33-year-old woman is brought to the emergency department 15 minutes after being stabbed in the chest with a screwdriver. Given her vital signs of pulse 110/min, respirations 22/min, and blood pressure 90/65 mm Hg, along with the presence of a 5-cm deep stab wound at the upper border of the 8th rib in the left midaxillary line, which anatomical structure in her chest is most likely to be injured?',\n 'Complex_CoT': \"Okay, let's figure out what's going on here. A woman comes in with a stab wound from a screwdriver. It's in her chest, upper border of the 8th rib, left side, kind of around the midaxillary line. First thought, that's pretty close to where the lung sits, right?\\n\\nLet's talk about location first. This spot is along the left side of her body. Above the 8th rib, like that, is where a lot of important stuff lives, like the bottom part of the left lung, possibly the diaphragm too, especially considering how deep the screwdriver went.\\n\\nThe wound is 5 cm deep. That sounds pretty deep. I mean, it could definitely reach down to the lung tissue or maybe the diaphragm. Given that it's midaxillary, we're in the territory where the lower lobe of the left lung hangs out. It's also possible there's some intersection with where the diaphragm begins, but the lung feels more probable somehow.\\n\\nNow, her vitals are concerning: elevated heart rate and low blood pressure. This is serious. My gut says that this kind of vital sign picture could mean something like pneumothorax or maybe hemothorax. Both can happen if the lung gets punctured, and they can make the blood pressure drop and the heart rate skyrocket since she's obviously distressed.\\n\\nSo, putting it all together, the most obvious culprit is the lower lobe of the left lung. The wound's depth and her condition point that way. And, yeah, this adds up with pneumothorax or maybe blood collecting in the chest—the kind of stuff that can really mess with breathing and circulation.\\n\\nAlright, with this in mind, it sure seems like the lung is the most likely thing that got hurt here. Makes sense given the situation, where the wound is, considering her symptoms, and the whole setup.\",\n 'Response': 'In this scenario, the most likely anatomical structure to be injured is the lower lobe of the left lung. The location of the stab wound—at the upper border of the 8th rib in the left midaxillary line—indicates proximity to the lower lobe of the lung. The depth of the wound (5 cm) suggests it is sufficient to reach lung tissue. Her vital signs of elevated heart rate and low blood pressure could signal complications like a pneumothorax or hemothorax, common consequences of lung trauma that would result from a penetrating injury in this area. Given these considerations, the lower lobe of the left lung is the most probable structure injured.'}"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# We need to format the dataset to fit our prompt training style \nEOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which the model when to stop generating text during training\nEOS_TOKEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:21:49.882626Z","iopub.execute_input":"2025-05-01T16:21:49.882977Z","iopub.status.idle":"2025-05-01T16:21:49.889261Z","shell.execute_reply.started":"2025-05-01T16:21:49.882949Z","shell.execute_reply":"2025-05-01T16:21:49.888426Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'<|end_of_text|>'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Define formatting prompt function\ndef formatting_prompts_func(examples):  # Takes a batch of dataset examples as input\n    inputs = examples[\"Question\"]       # Extracts the medical question from the dataset\n    cots = examples[\"Complex_CoT\"]      # Extracts the chain-of-thought reasoning (logical step-by-step explanation)\n    outputs = examples[\"Response\"]      # Extracts the final model-generated response (answer)\n    \n    texts = []  # Initializes an empty list to store the formatted prompts\n    \n    # Iterate over the dataset, formatting each question, reasoning step, and response\n    for input, cot, output in zip(inputs, cots, outputs):  \n        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN  # Insert values into prompt template & append EOS token\n        texts.append(text)  # Add the formatted text to the list\n\n    return {\n        \"text\": texts,  # Return the newly formatted dataset with a \"text\" column containing structured prompts\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:21:50.774754Z","iopub.execute_input":"2025-05-01T16:21:50.775092Z","iopub.status.idle":"2025-05-01T16:21:50.781146Z","shell.execute_reply.started":"2025-05-01T16:21:50.775067Z","shell.execute_reply":"2025-05-01T16:21:50.780345Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Update dataset formatting\ndataset_finetune = dataset.map(formatting_prompts_func, batched = True)\ndataset_finetune[\"text\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:21:53.821772Z","iopub.execute_input":"2025-05-01T16:21:53.822218Z","iopub.status.idle":"2025-05-01T16:21:53.887481Z","shell.execute_reply.started":"2025-05-01T16:21:53.822177Z","shell.execute_reply":"2025-05-01T16:21:53.886560Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d0dd9d79e8d477085f671a890dc1b2e"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nGiven the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\\n\\n### Response:\\n<think>\\nOkay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!\\n</think>\\nThe specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.<|end_of_text|>\""},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Apply LoRA (Low-Rank Adaptation) fine-tuning to the model \nmodel_lora = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA rank: Determines the size of the trainable adapters (higher = more parameters, lower = more efficiency)\n    target_modules=[  # List of transformer layers where LoRA adapters will be applied\n        \"q_proj\",   # Query projection in the self-attention mechanism\n        \"k_proj\",   # Key projection in the self-attention mechanism\n        \"v_proj\",   # Value projection in the self-attention mechanism\n        \"o_proj\",   # Output projection from the attention layer\n        \"gate_proj\",  # Used in feed-forward layers (MLP)\n        \"up_proj\",    # Part of the transformer’s feed-forward network (FFN)\n        \"down_proj\",  # Another part of the transformer’s FFN\n    ],\n    lora_alpha=16,  # Scaling factor for LoRA updates (higher values allow more influence from LoRA layers)\n    lora_dropout=0,  # Dropout rate for LoRA layers (0 means no dropout, full retention of information)\n    bias=\"none\",  # Specifies whether LoRA layers should learn bias terms (setting to \"none\" saves memory)\n    use_gradient_checkpointing=\"unsloth\",  # Saves memory by recomputing activations instead of storing them (recommended for long-context fine-tuning)\n    random_state=3407,  # Sets a seed for reproducibility, ensuring the same fine-tuning behavior across runs\n    use_rslora=False,  # Whether to use Rank-Stabilized LoRA (disabled here, meaning fixed-rank LoRA is used)\n    loftq_config=None,  # Low-bit Fine-Tuning Quantization (LoFTQ) is disabled in this configuration\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:21:57.063663Z","iopub.execute_input":"2025-05-01T16:21:57.063988Z","iopub.status.idle":"2025-05-01T16:22:03.485384Z","shell.execute_reply.started":"2025-05-01T16:21:57.063960Z","shell.execute_reply":"2025-05-01T16:22:03.484711Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.4.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Initialize the fine-tuning trainer — Imported using from trl import SFTTrainer\ntrainer = SFTTrainer(\n    model=model_lora,  # The model to be fine-tuned\n    tokenizer=tokenizer,  # Tokenizer to process text inputs\n    train_dataset=dataset_finetune,  # Dataset used for training\n    dataset_text_field=\"text\",  # Specifies which field in the dataset contains training text\n    max_seq_length=max_seq_length,  # Defines the maximum sequence length for inputs\n    dataset_num_proc=2,  # Uses 2 CPU threads to speed up data preprocessing\n\n    # Define training arguments\n    args=TrainingArguments(\n        per_device_train_batch_size=2,  # Number of examples processed per device (GPU) at a time\n        gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps before updating weights\n        num_train_epochs=3, # Full fine-tuning run\n        warmup_steps=5,  # Gradually increases learning rate for the first 5 steps\n        max_steps=70,  # Limits training to 60 steps (useful for debugging; increase for full fine-tuning)\n        learning_rate=2e-4,  # Learning rate for weight updates (tuned for LoRA fine-tuning)\n        fp16=not is_bfloat16_supported(),  # Use FP16 (if BF16 is not supported) to speed up training\n        bf16=is_bfloat16_supported(),  # Use BF16 if supported (better numerical stability on newer GPUs)\n        logging_steps=10,  # Logs training progress every 10 steps\n        optim=\"adamw_8bit\",  # Uses memory-efficient AdamW optimizer in 8-bit mode\n        weight_decay=0.01,  # Regularization to prevent overfitting\n        lr_scheduler_type=\"linear\",  # Uses a linear learning rate schedule\n        seed=3407,  # Sets a fixed seed for reproducibility\n        output_dir=\"outputs\",  # Directory where fine-tuned model checkpoints will be saved\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:22:06.997040Z","iopub.execute_input":"2025-05-01T16:22:06.997364Z","iopub.status.idle":"2025-05-01T16:22:09.702710Z","shell.execute_reply.started":"2025-05-01T16:22:06.997337Z","shell.execute_reply":"2025-05-01T16:22:09.702022Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fca4359cef147cc88295dbb9cb73705"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:22:13.176448Z","iopub.execute_input":"2025-05-01T16:22:13.176761Z","iopub.status.idle":"2025-05-01T16:22:13.182051Z","shell.execute_reply.started":"2025-05-01T16:22:13.176738Z","shell.execute_reply":"2025-05-01T16:22:13.181300Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Start the fine-tuning process\ntrainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:22:14.836955Z","iopub.execute_input":"2025-05-01T16:22:14.837259Z","iopub.status.idle":"2025-05-01T17:10:25.668509Z","shell.execute_reply.started":"2025-05-01T16:22:14.837236Z","shell.execute_reply":"2025-05-01T17:10:25.667845Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 2 | Total steps = 70\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [70/70 47:20, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.666900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.298200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.287000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.257400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.288200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.264800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.213400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Save the fine-tuned model\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:54:23.253608Z","iopub.execute_input":"2025-02-04T14:54:23.253849Z","iopub.status.idle":"2025-02-04T14:54:24.895075Z","shell.execute_reply.started":"2025-02-04T14:54:23.253827Z","shell.execute_reply":"2025-02-04T14:54:24.894333Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▃▅▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▅▆▇██</td></tr><tr><td>train/grad_norm</td><td>█▂▁▁▂▂▂</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>2.082437395243008e+16</td></tr><tr><td>train/epoch</td><td>1.112</td></tr><tr><td>train/global_step</td><td>70</td></tr><tr><td>train/grad_norm</td><td>0.27855</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.2428</td></tr><tr><td>train_loss</td><td>1.33008</td></tr><tr><td>train_runtime</td><td>1630.2135</td></tr><tr><td>train_samples_per_second</td><td>0.344</td></tr><tr><td>train_steps_per_second</td><td>0.043</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">glad-donkey-1</strong> at: <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/meta-llama-Llama-3.1-8B/runs/7lfkg2h0' target=\"_blank\">https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/meta-llama-Llama-3.1-8B/runs/7lfkg2h0</a><br> View project at: <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/meta-llama-Llama-3.1-8B' target=\"_blank\">https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/meta-llama-Llama-3.1-8B</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250204_141636-7lfkg2h0/logs</code>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer\n\ndef compute_perplexity(model, tokenizer, text):\n    \"\"\"\n    Computes perplexity (PPL) of a given text using a fine-tuned LoRA model.\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)\n\n    # Tokenize input text\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n    input_ids = inputs.input_ids.to(device)\n    attention_mask = inputs.attention_mask.to(device)\n\n    with torch.no_grad():  # Disable gradient computation\n        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n        loss = outputs.loss  # Get cross-entropy loss\n\n    # Compute perplexity\n    perplexity = torch.exp(loss).item()\n    return perplexity\n\n# Example usage\ntext_sample = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue, \n              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative, \n              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium. \n              What is the most likely predisposing factor for this patient's condition?\"\"\"\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/outputs/checkpoint-70\")  # Use the same tokenizer as during training\nppl = compute_perplexity(model_lora, tokenizer, text_sample)\n\nprint(f\"Perplexity: {ppl:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:34:49.671125Z","iopub.execute_input":"2025-05-01T17:34:49.671469Z","iopub.status.idle":"2025-05-01T17:34:50.761785Z","shell.execute_reply.started":"2025-05-01T17:34:49.671446Z","shell.execute_reply":"2025-05-01T17:34:50.760791Z"}},"outputs":[{"name":"stdout","text":"Perplexity: 6.66\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing \n              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, \n              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n\n# Load the inference model using FastLanguageModel (Unsloth optimizes for speed)\nFastLanguageModel.for_inference(model_lora)  # Unsloth has 2x faster inference!\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=1200,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T18:00:41.782536Z","iopub.execute_input":"2025-05-01T18:00:41.782894Z","iopub.status.idle":"2025-05-01T18:01:10.790414Z","shell.execute_reply.started":"2025-05-01T18:00:41.782869Z","shell.execute_reply":"2025-05-01T18:01:10.789614Z"}},"outputs":[{"name":"stdout","text":"\n<think> \nAlright, let's think this through. We have a 61-year-old woman who's dealing with involuntary urine loss when she coughs or sneezes. That's pretty common, especially as people age. It's called stress incontinence, right? It's when the bladder pressure goes up and the sphincter can't hold everything back. So, she's not leaking at night, which is interesting. That's usually a sign of something different, like overactive bladder or nocturia. \n\nNow, she's going through a gynecological exam, and they're doing a Q-tip test. That's usually to check how much the bladder is tilting forward, which can affect the urethra and lead to leakage. If it's tilting too much, it could be a sign of something called urethral hypermobility. \n\nBut what's really key here is that they're thinking about doing a cystometry. This test is like the gold standard for looking at how the bladder works. It measures things like how much urine is left in the bladder after voiding, and it also shows how the bladder muscle contracts. \n\nSo, let's think about her residual volume. If she has stress incontinence, her bladder might not be emptying completely. That would mean she'd have a higher residual volume, which is bad because it could lead to more infections. \n\nNow, for detrusor contractions, if she has stress incontinence, her bladder might be contracting too strongly when she coughs or sneezes. This is because the pressure inside the bladder goes up, and if the sphincter can't handle it, she leaks. So, we might see strong contractions in the cystometry. \n\nOverall, given her symptoms and the findings from the exam, I'd expect her cystometry to show a high residual volume and strong detrusor contractions. These are classic signs of stress incontinence, especially if she's not leaking at night. It all fits together nicely. \n</think>\nBased on the given scenario, the 61-year-old woman with stress incontinence is likely to have a high residual volume and strong detrusor contractions during cystometry. This is consistent with her symptoms of involuntary urine loss during activities like coughing or sneezing, but not at night. Stress incontinence is characterized by incomplete bladder emptying and strong detrusor contractions in response to increased bladder pressure. The cystometry results would confirm these findings.<|end_of_text|>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"question = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue, \n              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative, \n              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium. \n              What is the most likely predisposing factor for this patient's condition?\"\"\"\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=1200,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T18:01:29.802202Z","iopub.execute_input":"2025-05-01T18:01:29.802513Z","iopub.status.idle":"2025-05-01T18:01:55.870084Z","shell.execute_reply.started":"2025-05-01T18:01:29.802490Z","shell.execute_reply":"2025-05-01T18:01:55.869288Z"}},"outputs":[{"name":"stdout","text":"\n<think>Okay, let's think about this. We've got a guy who's 59 years old and he's got a fever, chills, night sweats, and he's really tired. That sounds like he's got an infection. Hmm, he's got a vegetation on his aortic valve, which is pretty serious. It's probably a sign of endocarditis. Now, the bacteria they found in his blood culture are gram-positive cocci in chains. That's interesting because it sounds like strep. But then, they don't grow in a medium with 6.5% salt. That's weird because strep usually grows in that. So, what's going on here? \n\nNow, let's think about the valve. He's got a vegetation, which usually happens when there's bacteria on the heart valves. That makes me think about the heart valve being damaged or not functioning properly. But why would that happen? Oh, wait, he's 59. That's a bit older, and with age, the heart valves can become more brittle and less flexible. They might not close as well, leading to bacteria getting stuck there. \n\nSo, if the heart valves are damaged, it could be due to a condition like rheumatic heart disease. That's common in older people. It's from a past infection, usually strep, that damaged the valves. And, if he had that, it would explain why he's got this strep infection now. It's like the bacteria are coming back to haunt him. \n\nSo, yeah, I think the most likely predisposing factor for his condition is rheumatic heart disease. That makes sense, especially since he's older and had a history of strep infections. It all ties together pretty well.</think>\nThe most likely predisposing factor for this patient's condition, given the symptoms, the presence of a vegetation on the aortic valve, and the characteristics of the bacteria in the blood culture, is rheumatic heart disease. This condition is known to cause damage to the heart valves, making them more susceptible to bacterial infections such as endocarditis. The patient's history of strep infections, combined with his age, suggests a past history of rheumatic heart disease, which increases the risk of this type of infection.<|end_of_text|>\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}