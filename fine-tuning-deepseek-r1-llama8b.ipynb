{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Credits\n\nThis notebook and this code is a fork of Abid Ali Awan's tutorial on [Fine-Tuning DeepSeek R1](https://www.datacamp.com/tutorial/fine-tuning-deepseek-r1-reasoning-model). Eternally grateful for his work with DataCamp and his contributions to the community.\n\n\n## Which tools & packages will we be using today?\n\nPackages we're going to be using throughout this walkthrough will be\n\n- `unsloth`: Efficient fine-tuning and inference for LLMs — Specifically we will be using:\n    - `FastLanguageModel` module to optimize inference & fine-tuning\n    - `get_peft_model` to enable LoRa (Low-Rank Adaptation) fine-tuning\n- `peft`: Supports LoRA-based fine-tuning for large models.\n- Different Hugging Face modules:\n    - `transformers` from HuggingFace to work with our fine-tuning data and handle different model tasks\n    - `trl` Transformer Reinforcement Learning from HuggingFace which allows for supervised fine-tuning of the model — we will use the `SFFTrainer` wrapper\n    - `datasets` to fetch reasoning datasets from the Hugging Face Hub\n- `torch`: Deep learning framework used for training\n- `wandb`: Provides access to weights and biases for tracking our fine-tuning experiment ","metadata":{}},{"cell_type":"code","source":"%%capture\n\n!pip install unsloth # install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:09:24.988332Z","iopub.execute_input":"2025-05-01T15:09:24.988625Z","iopub.status.idle":"2025-05-01T15:12:14.920310Z","shell.execute_reply.started":"2025-05-01T15:09:24.988602Z","shell.execute_reply":"2025-05-01T15:12:14.919377Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n# Hugging Face modules\nfrom huggingface_hub import login # Lets you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:12:19.955037Z","iopub.execute_input":"2025-05-01T15:12:19.955323Z","iopub.status.idle":"2025-05-01T15:12:48.119286Z","shell.execute_reply.started":"2025-05-01T15:12:19.955300Z","shell.execute_reply":"2025-05-01T15:12:48.118593Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Initialize Hugging Face & WnB tokens\nuser_secrets = UserSecretsClient() # from kaggle_secrets import UserSecretsClient\nhugging_face_token = user_secrets.get_secret(\"HuggingfaceAPI\")\nwnb_token = user_secrets.get_secret(\"w&b\")\n\n# Login to Hugging Face\nlogin(hugging_face_token) # from huggingface_hub import login\n\n# Login to WnB\nwandb.login(key=wnb_token) # import wandb\nrun = wandb.init(\n    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset_YouTube Walkthrough', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:12:48.120178Z","iopub.execute_input":"2025-05-01T15:12:48.120375Z","iopub.status.idle":"2025-05-01T15:13:01.252908Z","shell.execute_reply.started":"2025-05-01T15:12:48.120357Z","shell.execute_reply":"2025-05-01T15:13:01.252036Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mali408mehmood\u001b[0m (\u001b[33mali408mehmood-ghulam-ishaq-khan-institute-of-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250501_151254-6acx2hpm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough/runs/6acx2hpm' target=\"_blank\">royal-shape-10</a></strong> to <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough' target=\"_blank\">https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough/runs/6acx2hpm' target=\"_blank\">https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough/runs/6acx2hpm</a>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Loading DeepSeek R1 and the Tokenizer\n\n**What are we doing in this step?**\n\nIn this step, we **load the DeepSeek R1 model and its tokenizer** using `FastLanguageModel.from_pretrained()`. We also **configure key parameters** for efficient inference and fine-tuning. We will be using a distilled 8B version of R1 for faster computation.  \n\n**Key parameters explained**\n```py\nmax_seq_length = 2048  # Define the maximum sequence length a model can handle (i.e., number of tokens per input)\ndtype = None  # Default data type (usually auto-detected)\nload_in_4bit = True  # Enables 4-bit quantization – a memory-saving optimization\n```\n\n**Intuition behind 4-bit quantization**\n\nImagine compressing a **high-resolution image** to a smaller size—**it takes up less space but still looks good enough**. Similarly, **4-bit quantization reduces the precision of model weights**, making the model **smaller and faster while keeping most of its accuracy**. Instead of storing precise **32-bit or 16-bit numbers**, we compress them into **4-bit values**. This allows **large language models to run efficiently on consumer GPUs** without needing massive amounts of memory. ","metadata":{}},{"cell_type":"code","source":"# Set parameters\nmax_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # Set to default \nload_in_4bit = True # Enables 4 bit quantization — a memory saving optimization \n\n# Load the DeepSeek R1 model and tokenizer using unsloth — imported using: from unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",  # Load the pre-trained DeepSeek R1 model (8B parameter version)\n    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n    token=hugging_face_token, # Use hugging face token\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:13:26.112252Z","iopub.execute_input":"2025-05-01T15:13:26.112551Z","iopub.status.idle":"2025-05-01T15:14:24.580837Z","shell.execute_reply.started":"2025-05-01T15:13:26.112527Z","shell.execute_reply":"2025-05-01T15:14:24.580114Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.4.4: Fast Llama patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f41b8cb217af4cdaa5dcca17f6d39935"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e7397265f646e3bd236f533e746386"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"478c65b495744ff6bd0696e21504bfb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fba5fe610d84423a60536cebba1fa34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2fb765a77e341bcbdecf4474ff1297c"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Testing DeepSeek R1 on a medical use-case before fine-tuning\n\n\n### Defining a system prompt \nTo create a prompt style for the model, we will define a system prompt and include placeholders for the question and response generation. The prompt will guide the model to think step-by-step and provide a logical, accurate response.","metadata":{}},{"cell_type":"code","source":"# Define a system prompt under prompt_style \nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:14:24.581736Z","iopub.execute_input":"2025-05-01T15:14:24.582019Z","iopub.status.idle":"2025-05-01T15:14:24.586468Z","shell.execute_reply.started":"2025-05-01T15:14:24.581997Z","shell.execute_reply":"2025-05-01T15:14:24.585851Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Running inference on the model\n\nIn this step, we **test the DeepSeek R1 model** by providing a **medical question** and generating a response.  \nThe process involves the following steps:\n\n1. **Define a test question** related to a medical case.\n2. **Format the question using the structured prompt (`prompt_style`)** to ensure the model follows a logical reasoning process.\n3. **Tokenize the input and move it to the GPU (`cuda`)** for faster inference.\n4. **Generate a response using the model**, specifying key parameters like `max_new_tokens=1200` (limits response length).\n5. **Decode the output tokens back into text** to obtain the final readable answer.","metadata":{}},{"cell_type":"code","source":"# Creating a test medical question for inference\nquestion = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or \n              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, \n              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n\n# Enable optimized inference mode for Unsloth models (improves speed and efficiency)\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n\n# Format the question using the structured prompt (`prompt_style`) and tokenize it\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  # Convert input to PyTorch tensor & move to GPU\n\n# Generate a response using the model\noutputs = model.generate(\n    input_ids=inputs.input_ids, # Tokenized input question\n    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n    max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n    use_cache=True, # Enable caching for faster inference\n)\n\n# Decode the generated output tokens into human-readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the relevant response part (after \"### Response:\")\nprint(response[0].split(\"### Response:\")[1])  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:14:24.587948Z","iopub.execute_input":"2025-05-01T15:14:24.588155Z","iopub.status.idle":"2025-05-01T15:14:55.697075Z","shell.execute_reply.started":"2025-05-01T15:14:24.588135Z","shell.execute_reply":"2025-05-01T15:14:55.696055Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, so I'm trying to figure out what the cystometry would show for this 61-year-old woman. Let me start by going through the information given.\n\nShe has a history of involuntary urine loss when she coughs or sneezes but doesn't leak at night. That makes me think about stress urinary incontinence. I remember that stress incontinence is often due to the bladder not fully emptying, especially when there's increased pressure like from coughing.\n\nShe underwent a gynecological exam and Q-tip test. I'm not too familiar with the Q-tip test, but I think it's used to check for urethral obstruction. If the Q-tip doesn't stay at 45 degrees, it might mean that the urethra is blocked, which can cause difficulty in emptying the bladder. So maybe the Q-tip test showed some issue there.\n\nNow, the question is about what cystometry would reveal regarding residual volume and detrusor contractions. Cystometry is a test where they fill the bladder and measure how much is left after the patient can't urinate, and also check the contractions of the detrusor muscle.\n\nIf she has stress incontinence, I think her residual volume might be normal because she doesn't leak at night. But the detrusor contractions might be weak. If the detrusor isn't contracting well, the bladder doesn't empty completely, leading to the small amounts of urine loss during activities like coughing.\n\nWait, but if the Q-tip test showed obstruction, then maybe the issue is more about the urethra. In that case, the residual volume might be normal, but the detrusor contractions might be normal too. Hmm, I'm a bit confused.\n\nLet me think again. Stress incontinence is usually due to the bladder not emptying completely. So if the detrusor isn't contracting well, the residual volume would be high, but since she doesn't leak at night, maybe it's not that. Alternatively, if there's urethral obstruction, the bladder might be full, but she can't empty it, leading to retention.\n\nWait, but she does have involuntary loss, so it's more likely that the bladder isn't emptying. So maybe the detrusor contractions are weak, leading to residual volume being high, but she can still hold some urine, hence no leakage at night.\n\nSo putting it together, the cystometry would probably show a normal residual volume (since she doesn't leak at night) and weak detrusor contractions, indicating that the bladder doesn't empty completely during activities.\n</think>\n\nThe cystometry would most likely reveal a **normal residual volume** and **weak detrusor contractions**. This suggests that the bladder does not empty completely during activities like coughing, leading to the involuntary urine loss observed. The normal residual volume indicates that the bladder can hold some urine without leakage at night, while the weak detrusor contractions imply that the bladder muscle does not contract sufficiently to fully empty the bladder.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":">**Before starting fine-tuning — why are we fine-tuning in the first place?**\n>\n> Even without fine-tuning, our model successfully generated a chain of thought and provided reasoning before delivering the final answer. The reasoning process is encapsulated within the `<think>` `</think>` tags. So, why do we still need fine-tuning? The reasoning process, while detailed, was long-winded and not concise. Additionally, we want the final answer to be consistent in a certain style. \n\n","metadata":{}},{"cell_type":"markdown","source":"## Fine-tuning step by step\n\n### Step 1 — Update the system prompt \nWe will slightly change the prompt style for processing the dataset by adding the third placeholder for the complex chain of thought column. `</think>`","metadata":{}},{"cell_type":"code","source":"# Updated training prompt style to add </think> tag \ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:14:55.698111Z","iopub.execute_input":"2025-05-01T15:14:55.698379Z","iopub.status.idle":"2025-05-01T15:14:55.702691Z","shell.execute_reply.started":"2025-05-01T15:14:55.698355Z","shell.execute_reply":"2025-05-01T15:14:55.701738Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Step 2 — Download the fine-tuning dataset and format it for fine-tuning\n\nWe will use the Medical O1 Reasoninng SFT found here on [Hugging Face](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT). From the authors: This dataset is used to fine-tune HuatuoGPT-o1, a medical LLM designed for advanced medical reasoning. This dataset is constructed using GPT-4o, which searches for solutions to verifiable medical problems and validates them through a medical verifier.","metadata":{}},{"cell_type":"code","source":"# Download the dataset using Hugging Face — function imported using from datasets import load_dataset\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True) # Keep only first 500 rows\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:14:55.703695Z","iopub.execute_input":"2025-05-01T15:14:55.704022Z","iopub.status.idle":"2025-05-01T15:14:59.094755Z","shell.execute_reply.started":"2025-05-01T15:14:55.703991Z","shell.execute_reply":"2025-05-01T15:14:59.094140Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbbcb3377f4c4d0dbcf9b47756090016"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"medical_o1_sft.json:   0%|          | 0.00/58.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fbbe06762f84c75ae385249a82b4a7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/19704 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caabd8f36b2f4d9f946c2c5996aab401"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Question', 'Complex_CoT', 'Response'],\n    num_rows: 500\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Show an entry from the dataset\ndataset[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:15:11.625526Z","iopub.execute_input":"2025-05-01T15:15:11.625971Z","iopub.status.idle":"2025-05-01T15:15:11.635358Z","shell.execute_reply.started":"2025-05-01T15:15:11.625935Z","shell.execute_reply":"2025-05-01T15:15:11.634388Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'Question': 'A 33-year-old woman is brought to the emergency department 15 minutes after being stabbed in the chest with a screwdriver. Given her vital signs of pulse 110/min, respirations 22/min, and blood pressure 90/65 mm Hg, along with the presence of a 5-cm deep stab wound at the upper border of the 8th rib in the left midaxillary line, which anatomical structure in her chest is most likely to be injured?',\n 'Complex_CoT': \"Okay, let's figure out what's going on here. A woman comes in with a stab wound from a screwdriver. It's in her chest, upper border of the 8th rib, left side, kind of around the midaxillary line. First thought, that's pretty close to where the lung sits, right?\\n\\nLet's talk about location first. This spot is along the left side of her body. Above the 8th rib, like that, is where a lot of important stuff lives, like the bottom part of the left lung, possibly the diaphragm too, especially considering how deep the screwdriver went.\\n\\nThe wound is 5 cm deep. That sounds pretty deep. I mean, it could definitely reach down to the lung tissue or maybe the diaphragm. Given that it's midaxillary, we're in the territory where the lower lobe of the left lung hangs out. It's also possible there's some intersection with where the diaphragm begins, but the lung feels more probable somehow.\\n\\nNow, her vitals are concerning: elevated heart rate and low blood pressure. This is serious. My gut says that this kind of vital sign picture could mean something like pneumothorax or maybe hemothorax. Both can happen if the lung gets punctured, and they can make the blood pressure drop and the heart rate skyrocket since she's obviously distressed.\\n\\nSo, putting it all together, the most obvious culprit is the lower lobe of the left lung. The wound's depth and her condition point that way. And, yeah, this adds up with pneumothorax or maybe blood collecting in the chest—the kind of stuff that can really mess with breathing and circulation.\\n\\nAlright, with this in mind, it sure seems like the lung is the most likely thing that got hurt here. Makes sense given the situation, where the wound is, considering her symptoms, and the whole setup.\",\n 'Response': 'In this scenario, the most likely anatomical structure to be injured is the lower lobe of the left lung. The location of the stab wound—at the upper border of the 8th rib in the left midaxillary line—indicates proximity to the lower lobe of the lung. The depth of the wound (5 cm) suggests it is sufficient to reach lung tissue. Her vital signs of elevated heart rate and low blood pressure could signal complications like a pneumothorax or hemothorax, common consequences of lung trauma that would result from a penetrating injury in this area. Given these considerations, the lower lobe of the left lung is the most probable structure injured.'}"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":">**Next step is to structure the fine-tuning dataset according to train prompt style—why?**\n>\n> - Each question is paired with chain-of-thought reasoning and the final response.\n> - Ensures every training example follows a consistent pattern.\n> - Prevents the model from continuing beyond the expected response lengt by adding the EOS token.","metadata":{}},{"cell_type":"code","source":"# We need to format the dataset to fit our prompt training style \nEOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which the model when to stop generating text during training\nEOS_TOKEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:15:16.311670Z","iopub.execute_input":"2025-05-01T15:15:16.312024Z","iopub.status.idle":"2025-05-01T15:15:16.318261Z","shell.execute_reply.started":"2025-05-01T15:15:16.311995Z","shell.execute_reply":"2025-05-01T15:15:16.317379Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'<｜end▁of▁sentence｜>'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Define formatting prompt function\ndef formatting_prompts_func(examples):  # Takes a batch of dataset examples as input\n    inputs = examples[\"Question\"]       # Extracts the medical question from the dataset\n    cots = examples[\"Complex_CoT\"]      # Extracts the chain-of-thought reasoning (logical step-by-step explanation)\n    outputs = examples[\"Response\"]      # Extracts the final model-generated response (answer)\n    \n    texts = []  # Initializes an empty list to store the formatted prompts\n    \n    # Iterate over the dataset, formatting each question, reasoning step, and response\n    for input, cot, output in zip(inputs, cots, outputs):  \n        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN  # Insert values into prompt template & append EOS token\n        texts.append(text)  # Add the formatted text to the list\n\n    return {\n        \"text\": texts,  # Return the newly formatted dataset with a \"text\" column containing structured prompts\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:15:17.247534Z","iopub.execute_input":"2025-05-01T15:15:17.247999Z","iopub.status.idle":"2025-05-01T15:15:17.255441Z","shell.execute_reply.started":"2025-05-01T15:15:17.247958Z","shell.execute_reply":"2025-05-01T15:15:17.254351Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Update dataset formatting\ndataset_finetune = dataset.map(formatting_prompts_func, batched = True)\ndataset_finetune[\"text\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:15:20.908098Z","iopub.execute_input":"2025-05-01T15:15:20.908385Z","iopub.status.idle":"2025-05-01T15:15:20.958331Z","shell.execute_reply.started":"2025-05-01T15:15:20.908363Z","shell.execute_reply":"2025-05-01T15:15:20.957640Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d2646edebf416c97089e98af60e067"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nGiven the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\\n\\n### Response:\\n<think>\\nOkay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!\\n</think>\\nThe specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.<｜end▁of▁sentence｜>\""},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"### Step 3 — Setting up the model using LoRA\n\n**An intuitive explanation of LoRA** \n\nLarge language models (LLMs) have **millions or even billions of weights** that determine how they process and generate text. When fine-tuning a model, we usually update all these weights, which **requires massive computational resources and memory**.\n\nLoRA (**Low-Rank Adaptation**) allows to fine-tune efficiently by:\n\n- Instead of modifying all weights, **LoRA adds small, trainable adapters** to specific layers.  \n- These adapters **capture task-specific knowledge** while leaving the original model unchanged.  \n- This reduces the number of trainable parameters **by more than 90%**, making fine-tuning **faster and more memory-efficient**.  \n\nThink of an LLM as a **complex factory**. Instead of rebuilding the entire factory to produce a new product, LoRA **adds small, specialized tools** to existing machines. This allows the factory to adapt quickly **without disrupting its core structure**.\n\nFor a more technical explanation, check out this tutorial by [Sebastian Raschka](https://www.youtube.com/watch?v=rgmJep4Sb4&t).\n\nBelow, we will use the `get_peft_model()` function which stands for Parameter-Efficient Fine-Tuning — this function wraps the base model (`model`) with LoRA modifications, ensuring that only specific parameters are trained.","metadata":{}},{"cell_type":"code","source":"# Apply LoRA (Low-Rank Adaptation) fine-tuning to the model \nmodel_lora = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA rank: Determines the size of the trainable adapters (higher = more parameters, lower = more efficiency)\n    target_modules=[  # List of transformer layers where LoRA adapters will be applied\n        \"q_proj\",   # Query projection in the self-attention mechanism\n        \"k_proj\",   # Key projection in the self-attention mechanism\n        \"v_proj\",   # Value projection in the self-attention mechanism\n        \"o_proj\",   # Output projection from the attention layer\n        \"gate_proj\",  # Used in feed-forward layers (MLP)\n        \"up_proj\",    # Part of the transformer’s feed-forward network (FFN)\n        \"down_proj\",  # Another part of the transformer’s FFN\n    ],\n    lora_alpha=16,  # Scaling factor for LoRA updates (higher values allow more influence from LoRA layers)\n    lora_dropout=0,  # Dropout rate for LoRA layers (0 means no dropout, full retention of information)\n    bias=\"none\",  # Specifies whether LoRA layers should learn bias terms (setting to \"none\" saves memory)\n    use_gradient_checkpointing=\"unsloth\",  # Saves memory by recomputing activations instead of storing them (recommended for long-context fine-tuning)\n    random_state=3407,  # Sets a seed for reproducibility, ensuring the same fine-tuning behavior across runs\n    use_rslora=False,  # Whether to use Rank-Stabilized LoRA (disabled here, meaning fixed-rank LoRA is used)\n    loftq_config=None,  # Low-bit Fine-Tuning Quantization (LoFTQ) is disabled in this configuration\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:15:25.177660Z","iopub.execute_input":"2025-05-01T15:15:25.178035Z","iopub.status.idle":"2025-05-01T15:15:31.971150Z","shell.execute_reply.started":"2025-05-01T15:15:25.178003Z","shell.execute_reply":"2025-05-01T15:15:31.970219Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.4.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Initialize the fine-tuning trainer — Imported using from trl import SFTTrainer\ntrainer = SFTTrainer(\n    model=model_lora,  # The model to be fine-tuned\n    tokenizer=tokenizer,  # Tokenizer to process text inputs\n    train_dataset=dataset_finetune,  # Dataset used for training\n    dataset_text_field=\"text\",  # Specifies which field in the dataset contains training text\n    max_seq_length=max_seq_length,  # Defines the maximum sequence length for inputs\n    dataset_num_proc=2,  # Uses 2 CPU threads to speed up data preprocessing\n\n    # Define training arguments\n    args=TrainingArguments(\n        per_device_train_batch_size=2,  # Number of examples processed per device (GPU) at a time\n        gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps before updating weights\n        num_train_epochs=3, # Full fine-tuning run\n        warmup_steps=5,  # Gradually increases learning rate for the first 5 steps\n        max_steps=70,  # Limits training to 60 steps (useful for debugging; increase for full fine-tuning)\n        learning_rate=2e-4,  # Learning rate for weight updates (tuned for LoRA fine-tuning)\n        fp16=not is_bfloat16_supported(),  # Use FP16 (if BF16 is not supported) to speed up training\n        bf16=is_bfloat16_supported(),  # Use BF16 if supported (better numerical stability on newer GPUs)\n        logging_steps=10,  # Logs training progress every 10 steps\n        optim=\"adamw_8bit\",  # Uses memory-efficient AdamW optimizer in 8-bit mode\n        weight_decay=0.01,  # Regularization to prevent overfitting\n        lr_scheduler_type=\"linear\",  # Uses a linear learning rate schedule\n        seed=3407,  # Sets a fixed seed for reproducibility\n        output_dir=\"outputs\",  # Directory where fine-tuned model checkpoints will be saved\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:15:36.181239Z","iopub.execute_input":"2025-05-01T15:15:36.181537Z","iopub.status.idle":"2025-05-01T15:15:38.971539Z","shell.execute_reply.started":"2025-05-01T15:15:36.181512Z","shell.execute_reply":"2025-05-01T15:15:38.970482Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"226f38bc0ce64880b2e40196d073b86b"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Start the fine-tuning process\ntrainer_stats = trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:15:43.616808Z","iopub.execute_input":"2025-05-01T15:15:43.617194Z","iopub.status.idle":"2025-05-01T16:03:53.849368Z","shell.execute_reply.started":"2025-05-01T15:15:43.617157Z","shell.execute_reply":"2025-05-01T16:03:53.848684Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 2 | Total steps = 70\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [70/70 47:20, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.936900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.417000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.395900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.352000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.380800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.357000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.296200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Save the fine-tuned model\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:07:21.802602Z","iopub.execute_input":"2025-02-03T17:07:21.802896Z","iopub.status.idle":"2025-02-03T17:07:23.117126Z","shell.execute_reply.started":"2025-02-03T17:07:21.802875Z","shell.execute_reply":"2025-02-03T17:07:23.116229Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▄▅▇██</td></tr><tr><td>train/global_step</td><td>▁▂▄▅▇██</td></tr><tr><td>train/grad_norm</td><td>█▁▄▂▁▃</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.7897487341420544e+16</td></tr><tr><td>train/epoch</td><td>0.48</td></tr><tr><td>train/global_step</td><td>60</td></tr><tr><td>train/grad_norm</td><td>0.27202</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.3747</td></tr><tr><td>train_loss</td><td>1.45493</td></tr><tr><td>train_runtime</td><td>1294.3903</td></tr><tr><td>train_samples_per_second</td><td>0.371</td></tr><tr><td>train_steps_per_second</td><td>0.046</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cosmic-sunset-4</strong> at: <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough/runs/mnfjjia5' target=\"_blank\">https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough/runs/mnfjjia5</a><br> View project at: <a href='https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough' target=\"_blank\">https://wandb.ai/ali408mehmood-ghulam-ishaq-khan-institute-of-engineering/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250203_164207-mnfjjia5/logs</code>"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"## Step 5 — Run model inference after fine-tuning","metadata":{}},{"cell_type":"code","source":"question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing \n              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, \n              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n\n# Load the inference model using FastLanguageModel (Unsloth optimizes for speed)\nFastLanguageModel.for_inference(model_lora)  # Unsloth has 2x faster inference!\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=1200,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:04:03.270592Z","iopub.execute_input":"2025-05-01T16:04:03.270940Z","iopub.status.idle":"2025-05-01T16:04:36.493716Z","shell.execute_reply.started":"2025-05-01T16:04:03.270909Z","shell.execute_reply":"2025-05-01T16:04:36.492849Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, so let's break this down. We have a 61-year-old woman who's been dealing with involuntary urine loss during things like coughing or sneezing, but she's not having any problems at night. That's interesting because it usually points towards stress urinary incontinence rather than something like nocturia. \n\nWhen they mentioned the Q-tip test, I think they're referring to the urethral pressure profilometry test. This test can tell us a lot about the pressure in the urethra and how it changes during activities like coughing or sneezing. If the pressure is high, it might mean that the urethra is not closing properly, which can lead to leaks.\n\nNow, when they do a gynecological exam, they might check for pelvic organ prolapse. Pelvic organ prolapse can definitely contribute to stress urinary incontinence, especially if the bladder is bulging out. This can happen when the pelvic muscles aren't working well enough to hold everything in place.\n\nCystometry is another important step. It's a test that looks at how the bladder behaves under different conditions, like coughing or sneezing. It also measures the residual volume of urine in the bladder. If the residual volume is low, it means the bladder empties more completely, which is good. But if there's a lot of residual volume, it might mean there's some kind of obstruction or incomplete emptying.\n\nOh, and what about the detrusor contractions? These are the contractions of the detrusor muscle that help push the urine out. If the detrusor contractions are not happening well, it could mean there's some kind of nerve problem that's not allowing the muscles to contract properly. That's definitely something that can lead to incontinence.\n\nPutting it all together, I think what cystometry would most likely show is a low residual volume. This makes sense because if the woman has been experiencing leaks during activities like coughing, it's often due to stress incontinence, which means the bladder empties more completely. So, the residual volume would be low. As for the detrusor contractions, they might not be as strong, but the residual volume is the key thing here. \n\nYeah, I think that's it. Cystometry would most likely show a low residual volume, indicating that the bladder empties well, but it might also show some weak detrusor contractions, suggesting there's some nerve involvement affecting muscle contractions.\n</think>\nBased on the findings from the gynecological exam and Q-tip test, cystometry is likely to reveal a low residual volume in the bladder. This indicates that the bladder empties more completely, which is typical for stress urinary incontinence. Additionally, there might be evidence of weak detrusor contractions, suggesting some nerve involvement affecting muscle contractions.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"question = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue, \n              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative, \n              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium. \n              What is the most likely predisposing factor for this patient's condition?\"\"\"\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=1200,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:05:17.241733Z","iopub.execute_input":"2025-05-01T16:05:17.242093Z","iopub.status.idle":"2025-05-01T16:05:47.844501Z","shell.execute_reply.started":"2025-05-01T16:05:17.242067Z","shell.execute_reply":"2025-05-01T16:05:47.843341Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, let's think about this. The patient has a vegetation on the aortic valve, which sounds like an infection, maybe endocarditis. The symptoms he's showing – fever, chills, night sweats, and fatigue – are classic signs of an infection. \n\nNow, the blood cultures show gram-positive, catalase-negative, gamma-hemolytic cocci in chains. Hmm, that matches with the description of Streptococcus pneumoniae. These bacteria are known for their ability to cause infections, especially in the lungs and heart. \n\nOh, and it's important to note that they don't grow in a 6.5% NaCl medium. That's a clue, right? Because Streptococcus pneumoniae is a serotype that doesn't require NaCl for growth, unlike some other Streptococcus species like Streptococcus pyogenes. \n\nSo, we're probably dealing with Streptococcus pneumoniae here. Now, what's the big deal with this organism? Well, it's known for causing pneumonia, sepsis, and endocarditis. It's also a leading cause of community-acquired pneumonia, which is pretty serious.\n\nNow, thinking about the predisposing factors for Streptococcus pneumoniae infections, we usually look at things like advanced age, weakened immune systems, and chronic lung diseases. Oh, and there's also this thing about smoking. It's a risk factor because it damages the lungs and makes people more susceptible to infections.\n\nBut wait, let's not forget that the patient is 59 years old. That's definitely a risk factor on its own. Plus, if there's something like chronic lung disease or smoking history, that could be a big deal. \n\nOh, and I should also consider if there's any underlying condition that makes him more susceptible. For example, having a weakened immune system, maybe due to HIV or diabetes, could really increase his risk. \n\nIn this case, the patient's age is already a significant factor. But we should also think about whether he has any other conditions that could make him more vulnerable. Without more specific information, age is a key factor here.\n\nSo, putting it all together, his age is definitely a big risk factor. That's the most likely reason for his condition. Yeah, I think that's the main thing we're looking at here.\n</think>\nThe most likely predisposing factor for this patient's condition is his age, which is 59 years old. Advanced age is a significant risk factor for Streptococcus pneumoniae infections, as it can lead to weakened immune responses and increased susceptibility to infections.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"pip install torch transformers datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:05:54.564046Z","iopub.execute_input":"2025-05-01T16:05:54.564485Z","iopub.status.idle":"2025-05-01T16:05:58.790259Z","shell.execute_reply.started":"2025-05-01T16:05:54.564445Z","shell.execute_reply":"2025-05-01T16:05:58.789407Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.7.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch) (9.5.1.17)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch) (0.6.3)\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch) (2.26.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.3.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.0->torch) (75.1.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer\n\ndef compute_perplexity(model, tokenizer, text):\n    \"\"\"\n    Computes perplexity (PPL) of a given text using a fine-tuned LoRA model.\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)\n\n    # Tokenize input text\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n    input_ids = inputs.input_ids.to(device)\n    attention_mask = inputs.attention_mask.to(device)\n\n    with torch.no_grad():  # Disable gradient computation\n        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n        loss = outputs.loss  # Get cross-entropy loss\n\n    # Compute perplexity\n    perplexity = torch.exp(loss).item()\n    return perplexity\n\n# Example usage\ntext_sample = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue, \n              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative, \n              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium. \n              What is the most likely predisposing factor for this patient's condition?\"\"\"\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/outputs/checkpoint-70\")  # Use the same tokenizer as during training\nppl = compute_perplexity(model_lora, tokenizer, text_sample)\n\nprint(f\"Perplexity: {ppl:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:06:06.275557Z","iopub.execute_input":"2025-05-01T16:06:06.275950Z","iopub.status.idle":"2025-05-01T16:06:07.434223Z","shell.execute_reply.started":"2025-05-01T16:06:06.275915Z","shell.execute_reply":"2025-05-01T16:06:07.433375Z"}},"outputs":[{"name":"stdout","text":"Perplexity: 8.70\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}